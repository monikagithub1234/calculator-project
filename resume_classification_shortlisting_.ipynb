{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxfB9bSokr/WHtiWF7LOqY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monikagithub1234/calculator-project/blob/main/resume_classification_shortlisting_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Required Libraries**"
      ],
      "metadata": {
        "id": "NRXDVlsCffKs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzGT9oQZ_mqp",
        "outputId": "1542c6d3-230b-45d5-d781-180e8c0b9bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy scikit-learn tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Prepare Dataset**"
      ],
      "metadata": {
        "id": "Uh0Xdk2pf6eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/UpdatedResumeDataSet.csv\")  # Update path\n",
        "\n",
        "# Show sample\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhWxM0snANCm",
        "outputId": "a8442882-58e5-432b-be42-26cc9c210176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Category                                             Resume\n",
            "0  Data Science  Skills * Programming Languages: Python (pandas...\n",
            "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
            "2  Data Science  Areas of Interest Deep Learning, Control Syste...\n",
            "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
            "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess Text Data**"
      ],
      "metadata": {
        "id": "31kO8Cr1ewyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd # Make sure pandas is imported if this is a separate cell\n",
        "\n",
        "# Clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "    return text\n",
        "\n",
        "df['Cleaned_Resume'] = df['Resume'].apply(clean_text)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(max_features=3000)\n",
        "X = tfidf.fit_transform(df['Cleaned_Resume']).toarray()\n",
        "\n",
        "# Labels\n",
        "# Check the column names to find the correct one\n",
        "print(df.columns)\n",
        "\n",
        "# Once you know the correct column name, replace 'Shortlisted' below\n",
        "# For example, if the column is named 'shortlisted_status', change the line to:\n",
        "# y = df['shortlisted_status']\n",
        "# Assuming the correct column name is 'Category' based on the df variable output\n",
        "y = df['Category']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qFXhOG5BlFT",
        "outputId": "15289c90-c883-49a2-f01d-dd6b6c3bded5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Category', 'Resume', 'Cleaned_Resume'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/Test Split**"
      ],
      "metadata": {
        "id": "06iviPQfgNNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "2hnssWKsBune"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build ANN Model (Using Keras)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Binary output\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "P4xt-OPUB3KQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "325b72df-4c5d-41e2-96e5-6105e667558b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m384,128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">384,128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m392,449\u001b[0m (1.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">392,449</span> (1.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m392,449\u001b[0m (1.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">392,449</span> (1.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np # Import numpy if not already imported\n",
        "\n",
        "# Convert string labels to integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test) # Use transform on test set\n",
        "\n",
        "# One-hot encode the integer labels\n",
        "y_train_categorical = to_categorical(y_train_encoded)\n",
        "y_test_categorical = to_categorical(y_test_encoded)\n",
        "\n",
        "# Update the output layer of the model to match the number of classes\n",
        "num_classes = y_train_categorical.shape[1]\n",
        "# Rebuild the model with the correct output layer if it was already defined\n",
        "# Note: It's better to define the model *after* determining num_classes if possible\n",
        "# or modify the existing model if it's the only way in your notebook structure.\n",
        "# Since the model is defined in a previous cell, we'll redefine and recompile it here\n",
        "# for simplicity in this fix, but ideally, the model definition should happen\n",
        "# after determining `num_classes`.\n",
        "\n",
        "# Redefine and recompile the model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Use softmax for multiclass\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Now train the model with the one-hot encoded labels\n",
        "history = model.fit(X_train, y_train_categorical, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "XmA-JyEMC9TO",
        "outputId": "092621ed-e019-4704-c3d4-4a235bb14c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m384,128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m1,625\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">384,128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,625</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m394,009\u001b[0m (1.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">394,009</span> (1.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m394,009\u001b[0m (1.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">394,009</span> (1.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.3337 - loss: 3.1729 - val_accuracy: 0.5584 - val_loss: 2.9914\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7094 - loss: 2.7982 - val_accuracy: 0.5974 - val_loss: 2.4520\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7279 - loss: 2.0585 - val_accuracy: 0.7468 - val_loss: 1.7083\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8829 - loss: 1.2044 - val_accuracy: 0.9091 - val_loss: 1.0104\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9872 - loss: 0.5980 - val_accuracy: 0.9870 - val_loss: 0.5447\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.2872 - val_accuracy: 1.0000 - val_loss: 0.2933\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.1489 - val_accuracy: 1.0000 - val_loss: 0.1780\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0863 - val_accuracy: 1.0000 - val_loss: 0.1193\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0477 - val_accuracy: 1.0000 - val_loss: 0.0914\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0346 - val_accuracy: 1.0000 - val_loss: 0.0712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the model using the one-hot encoded test labels\n",
        "loss, accuracy = model.evaluate(X_test, y_test_categorical) # Use y_test_categorical instead of y_test\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKk1DeQKDbII",
        "outputId": "d1a983bd-ad37-4a8f-f73a-e7d9e961e74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9973 - loss: 0.0530 \n",
            "Test Accuracy: 99.48%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Make Predictions\n",
        "sample_resume = [\"I have experience in data science and Python\"]\n",
        "sample_clean = [clean_text(sample_resume[0])]\n",
        "sample_vec = tfidf.transform(sample_clean).toarray()\n",
        "prediction = model.predict(sample_vec)\n",
        "print(\"Shortlisted\" if prediction[0][0] > 0.5 else \"Not Shortlisted\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TndavQTZDire",
        "outputId": "0a8c742a-00bc-4ef2-a5d1-5c0c5cb23dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "Not Shortlisted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Save Model\n",
        "\n",
        "model.save(\"resume_shortlist_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "481rN0x7D1M1",
        "outputId": "de3ae460-805d-4ee2-9a70-ddcff2c8344a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**✅ Modified Output with Shortlist + Confidence Score**"
      ],
      "metadata": {
        "id": "LSZLn7mZgZ27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "# Ensure this path is correct for the dataset you intend to use\n",
        "df = pd.read_csv(\"/content/Resume.csv\")\n",
        "\n",
        "# Step 2: Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "\n",
        "df['Cleaned_Resume'] = df['Resume'].apply(clean_text)\n",
        "\n",
        "# Step 3: Vectorize text using TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=3000)\n",
        "X = tfidf.fit_transform(df['Cleaned_Resume']).toarray()\n",
        "\n",
        "# Step 4: Encode target labels\n",
        "le = LabelEncoder()\n",
        "# Change 'Category' to 'Shortlisted' to match the column in the loaded DataFrame\n",
        "y = le.fit_transform(df['Shortlisted'])\n",
        "\n",
        "# Optional: Print category mapping\n",
        "print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
        "\n",
        "# Step 5: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Build ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "# The number of output nodes should match the number of unique classes\n",
        "# Since we are using LabelEncoder on the 'Shortlisted' column which likely has two classes (Shortlisted/Not Shortlisted or similar)\n",
        "# the number of classes will be 2. Softmax activation is appropriate for multi-class classification.\n",
        "# Using len(le.classes_) ensures the output layer size matches the number of unique labels.\n",
        "model.add(Dense(len(le.classes_), activation='softmax'))\n",
        "\n",
        "# Use 'sparse_categorical_crossentropy' as the loss function when labels are integers (not one-hot encoded)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 7: Train model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Step 8: Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# Step 9: Predict on new input\n",
        "sample_resume = [\"Skilled in Python, data analysis, and machine learning\"]\n",
        "sample_clean = [clean_text(sample_resume[0])]\n",
        "sample_vec = tfidf.transform(sample_clean).toarray()\n",
        "\n",
        "pred = model.predict(sample_vec)\n",
        "# Inverse transform the predicted integer label back to its original string representation\n",
        "predicted_label = le.inverse_transform([pred.argmax()])[0]\n",
        "\n",
        "print(\"\\nPredicted Category:\", predicted_label)\n",
        "if predicted_label == 1:\n",
        "    print(\"Shortlisted ✅\")\n",
        "else:\n",
        "    print(\"Not Shortlisted ❌\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bld9fkzl1Jh",
        "outputId": "4f122729-8b67-481e-e65e-01abf4f6c23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label mapping: {np.int64(0): np.int64(0), np.int64(1): np.int64(1)}\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.6726 - val_accuracy: 0.0000e+00 - val_loss: 0.6988\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8333 - loss: 0.6490 - val_accuracy: 0.0000e+00 - val_loss: 0.6987\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 1.0000 - loss: 0.6265 - val_accuracy: 0.5000 - val_loss: 0.6990\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 1.0000 - loss: 0.6048 - val_accuracy: 0.5000 - val_loss: 0.6995\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 0.5835 - val_accuracy: 0.5000 - val_loss: 0.7002\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 0.5632 - val_accuracy: 0.5000 - val_loss: 0.7013\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 0.5438 - val_accuracy: 0.5000 - val_loss: 0.7026\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.5246 - val_accuracy: 0.5000 - val_loss: 0.7039\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 0.5053 - val_accuracy: 0.5000 - val_loss: 0.7052\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.4864 - val_accuracy: 0.5000 - val_loss: 0.7064\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6433\n",
            "\n",
            "Test Accuracy: 50.00%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "Predicted Category: 1\n",
            "Shortlisted ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a Skill Match Score (Bonus Idea)\n",
        "required_skills = ['python', 'sql', 'machine learning', 'data science']\n",
        "\n",
        "def skill_match_score(resume_text):\n",
        "    resume_text = clean_text(resume_text)\n",
        "    matched = [skill for skill in required_skills if skill in resume_text]\n",
        "    return len(matched), matched\n",
        "\n",
        "count, matched = skill_match_score(sample_resume[0])\n",
        "print(f\"Skills matched: {count} ➤ {matched}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYtdKCy1nzeI",
        "outputId": "69fa8e9b-63c8-4bad-f8e6-ae8122bdbfd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skills matched: 2 ➤ ['python', 'machine learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# my resume\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %% [markdown]\n",
        "# **Install Required Libraries**\n",
        "# %%\n",
        "!pip install pandas numpy scikit-learn tensorflow\n",
        "# %% [markdown]\n",
        "# **Load and Prepare Dataset**\n",
        "# %%\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "# Using the specified file path /content/Resume.csv\n",
        "try:\n",
        "    # *** IMPORTANT ***\n",
        "    # Ensure that the file at this path (/content/Resume.csv) actually\n",
        "    # contains the columns 'Resume' and 'Category'. If it doesn't,\n",
        "    # you will get a KeyError later.\n",
        "    df = pd.read_csv(\"/content/Resume.csv\")\n",
        "    print(\"Successfully loaded /content/Resume.csv\")\n",
        "    # Show sample\n",
        "    print(df.head())\n",
        "    print(\"\\nColumns:\", df.columns.tolist()) # Print column names to verify\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: /content/Resume.csv not found. Please check the file path.\")\n",
        "    # You might want to exit or handle this error appropriately\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the dataset from /content/Resume.csv: {e}\")\n",
        "    # Handle other potential loading errors\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# **Preprocess Text Data**\n",
        "# %%\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# pandas is already imported in the previous cell, but good practice to re-import if this were a standalone cell\n",
        "# import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder here as it's used in the next block\n",
        "import numpy as np # Import numpy as it's used later in prediction\n",
        "\n",
        "# Clean text function\n",
        "def clean_text(text):\n",
        "    # Handle potential NaN values in the text column\n",
        "    if isinstance(text, str):\n",
        "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "        return text\n",
        "    else:\n",
        "        return \"\" # Return an empty string for non-string inputs\n",
        "\n",
        "# Ensure df is loaded and 'Resume' column exists before processing\n",
        "if 'df' in locals() and 'Resume' in df.columns:\n",
        "    df['Cleaned_Resume'] = df['Resume'].apply(clean_text)\n",
        "    print(\"\\n'Cleaned_Resume' column created.\")\n",
        "    print(df[['Resume', 'Cleaned_Resume']].head()) # Show sample of cleaned data\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    tfidf = TfidfVectorizer(max_features=3000)\n",
        "\n",
        "    # Ensure 'Cleaned_Resume' column exists before vectorizing\n",
        "    if 'Cleaned_Resume' in df.columns:\n",
        "        X = tfidf.fit_transform(df['Cleaned_Resume']).toarray()\n",
        "        print(f\"\\nTF-IDF vectorization complete. Shape of X: {X.shape}\")\n",
        "    else:\n",
        "        print(\"Error: 'Cleaned_Resume' column not found after cleaning. Cannot perform TF-IDF vectorization.\")\n",
        "\n",
        "    # Labels\n",
        "    # Assuming 'Category' is the correct column name from the dataset you are using\n",
        "    if 'Category' in df.columns:\n",
        "        y = df['Category']\n",
        "        print(f\"\\n'Category' column loaded. Number of samples: {len(y)}\")\n",
        "\n",
        "        # Convert string labels to integers using LabelEncoder\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_encoded = label_encoder.fit_transform(y)\n",
        "        print(\"Labels encoded.\")\n",
        "\n",
        "        # Print label mapping to understand the integer representation of categories\n",
        "        print(\"Label mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
        "        print(f\"Shape of y_encoded: {y_encoded.shape}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Error: 'Category' column not found in the DataFrame. Cannot encode labels.\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: DataFrame not loaded or 'Resume' column missing. Skipping preprocessing.\")\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# **Train/Test Split**\n",
        "# %%\n",
        "# Ensure X and y_encoded were successfully created in the previous step\n",
        "if 'X' in locals() and 'y_encoded' in locals():\n",
        "    # Use the encoded labels for the train/test split\n",
        "    X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "    print(f\"\\nTrain/Test split complete. Shapes:\")\n",
        "    print(f\"  X_train: {X_train.shape}\")\n",
        "    print(f\"  X_test: {X_test.shape}\")\n",
        "    print(f\"  y_train_encoded: {y_train_encoded.shape}\")\n",
        "    print(f\"  y_test_encoded: {y_test_encoded.shape}\")\n",
        "else:\n",
        "    print(\"Error: X or y_encoded not defined. Skipping train/test split.\")\n",
        "\n",
        "\n",
        "# %%\n",
        "# Build ANN Model (Using Keras)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# numpy is already imported in the previous cell, but good practice to re-import if standalone\n",
        "# import numpy as np\n",
        "\n",
        "# Ensure label_encoder was successfully created\n",
        "if 'label_encoder' in locals():\n",
        "    # Determine the number of classes from the encoded labels\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    print(f\"\\nNumber of classes: {num_classes}\")\n",
        "\n",
        "    # Ensure y_train_encoded and y_test_encoded were successfully created\n",
        "    if 'y_train_encoded' in locals() and 'y_test_encoded' in locals():\n",
        "        # One-hot encode the integer labels\n",
        "        y_train_categorical = to_categorical(y_train_encoded, num_classes=num_classes)\n",
        "        y_test_categorical = to_categorical(y_test_encoded, num_classes=num_classes)\n",
        "        print(\"Labels one-hot encoded.\")\n",
        "        print(f\"  y_train_categorical: {y_train_categorical.shape}\")\n",
        "        print(f\"  y_test_categorical: {y_test_categorical.shape}\")\n",
        "\n",
        "        # Ensure X_train is defined for input shape\n",
        "        if 'X_train' in locals():\n",
        "            # ANN model\n",
        "            model = Sequential()\n",
        "            model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "            model.add(Dense(64, activation='relu'))\n",
        "            # The output layer should have 'num_classes' nodes with 'softmax' activation for multi-class classification\n",
        "            model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "            # Use 'categorical_crossentropy' for loss when using one-hot encoded labels\n",
        "            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "            print(\"\\nANN Model built and compiled.\")\n",
        "            model.summary()\n",
        "        else:\n",
        "            print(\"Error: X_train not defined. Cannot build model.\")\n",
        "    else:\n",
        "        print(\"Error: Encoded labels not defined. Cannot build model.\")\n",
        "else:\n",
        "    print(\"Error: label_encoder not defined. Cannot determine number of classes.\")\n",
        "\n",
        "\n",
        "# %%\n",
        "# Train the Model\n",
        "# Ensure the model and training data are defined\n",
        "if 'model' in locals() and 'X_train' in locals() and 'y_train_categorical' in locals():\n",
        "    print(\"\\nStarting model training...\")\n",
        "    history = model.fit(X_train, y_train_categorical, epochs=10, batch_size=32, validation_split=0.2)\n",
        "    print(\"Model training finished.\")\n",
        "else:\n",
        "    print(\"Error: Model or training data not defined. Cannot train model.\")\n",
        "\n",
        "# %%\n",
        "# Evaluate the model using the one-hot encoded test labels\n",
        "# Ensure the model and test data are defined\n",
        "if 'model' in locals() and 'X_test' in locals() and 'y_test_categorical' in locals():\n",
        "    print(\"\\nEvaluating model on test data...\")\n",
        "    loss, accuracy = model.evaluate(X_test, y_test_categorical)\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
        "else:\n",
        "    print(\"Error: Model or test data not defined. Cannot evaluate model.\")\n",
        "\n",
        "\n",
        "# %%\n",
        "#  Make Predictions\n",
        "# Ensure clean_text, tfidf, model, and label_encoder are defined\n",
        "if 'clean_text' in locals() and 'tfidf' in locals() and 'model' in locals() and 'label_encoder' in locals():\n",
        "    sample_resume = [\"I have experience in data science and Python\"]\n",
        "    print(f\"\\nSample resume for prediction: '{sample_resume[0]}'\")\n",
        "\n",
        "    sample_clean = [clean_text(sample_resume[0])]\n",
        "    print(f\"Cleaned sample resume: '{sample_clean[0]}'\")\n",
        "\n",
        "    # Ensure tfidf was fit on the training data and can transform new data\n",
        "    try:\n",
        "        sample_vec = tfidf.transform(sample_clean).toarray()\n",
        "        print(f\"Sample resume vectorized. Shape: {sample_vec.shape}\")\n",
        "\n",
        "        # Make prediction\n",
        "        prediction_probs = model.predict(sample_vec)\n",
        "        print(f\"Prediction probabilities (for all categories): {prediction_probs[0]}\") # Print probabilities for the first sample\n",
        "\n",
        "        # Get the predicted class index (the one with the highest probability)\n",
        "        predicted_class_index = np.argmax(prediction_probs, axis=1)[0]\n",
        "        print(f\"Predicted class index: {predicted_class_index}\")\n",
        "\n",
        "        # Inverse transform the predicted class index back to the original category name\n",
        "        predicted_category = label_encoder.inverse_transform([predicted_class_index])[0]\n",
        "\n",
        "        # Print the predicted category\n",
        "        print(f\"\\nPredicted Category: {predicted_category}\")\n",
        "\n",
        "        # --- ADD SHORTLISTING LOGIC HERE ---\n",
        "        # Define which categories are considered 'Shortlisted'\n",
        "        # *** IMPORTANT: EDIT THIS LIST ***\n",
        "        # Replace these example categories with the actual category names\n",
        "        # from your 'label_encoder.classes_' that you want to shortlist.\n",
        "        shortlisted_categories = ['Data Science', 'Machine Learning', 'Web Development', 'Python Developer'] # <-- **EDIT THIS LIST**\n",
        "\n",
        "        # Check if the predicted category is in the list of shortlisted categories\n",
        "        if predicted_category in shortlisted_categories:\n",
        "            print(\"Shortlisted ✅\")\n",
        "            # Optional: You could also get the confidence score for the predicted category\n",
        "            confidence_score = prediction_probs[0][predicted_class_index]\n",
        "            print(f\"Confidence Score for Predicted Category: {confidence_score:.4f}\")\n",
        "        else:\n",
        "            print(\"Not Shortlisted ❌\")\n",
        "\n",
        "        # --- END SHORTLISTING LOGIC ---\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during prediction: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: Required components for prediction (clean_text, tfidf, model, label_encoder) not defined.\")\n",
        "\n",
        "# %%\n",
        "#  Save Model\n",
        "# Ensure the model is defined\n",
        "if 'model' in locals():\n",
        "    # Ensure directory exists or save to a path where you have write permissions\n",
        "    try:\n",
        "        model.save(\"resume_classification_model.h5\")\n",
        "        print(\"\\nModel saved successfully as resume_classification_model.h5\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving model: {e}\")\n",
        "        print(\"Please ensure you have write permissions in the current directory.\")\n",
        "else:\n",
        "    print(\"\\nError: Model not defined. Cannot save model.\")\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# **Add a Skill Match Score (Bonus Idea)**\n",
        "# %%\n",
        "# Add a Skill Match Score (Bonus Idea)\n",
        "# Ensure clean_text is defined (it's defined in the main preprocessing section)\n",
        "if 'clean_text' in locals():\n",
        "    required_skills = ['python', 'sql', 'machine learning', 'data science']\n",
        "\n",
        "    def skill_match_score(resume_text):\n",
        "        # Handle potential non-string input\n",
        "        if isinstance(resume_text, str):\n",
        "            resume_text = clean_text(resume_text)\n",
        "            matched = [skill for skill in required_skills if skill in resume_text]\n",
        "            return len(matched), matched\n",
        "        else:\n",
        "            return 0, []\n",
        "\n",
        "\n",
        "    # Ensure sample_resume is defined (it is defined in the 'Make Predictions' cell)\n",
        "    if 'sample_resume' in locals():\n",
        "        print(\"\\nCalculating skill match for sample resume:\")\n",
        "        count, matched = skill_match_score(sample_resume[0])\n",
        "        print(f\"Skills matched: {count} ➤ {matched}\")\n",
        "    else:\n",
        "        print(\"\\nError: sample_resume not defined. Cannot calculate skill match score.\")\n",
        "else:\n",
        "     print(\"\\nError: clean_text function not defined. Cannot calculate skill match score.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "awRz38r3UDMl",
        "outputId": "875b36c1-39d8-4512-cbe7-b7af9732f6c3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Successfully loaded /content/Resume.csv\n",
            "  Unnamed: 0        Unnamed: 1  \\\n",
            "0        NaN               NaN   \n",
            "1        NaN               NaN   \n",
            "2        NaN               NaN   \n",
            "3       Year       Degree/Exam   \n",
            "4       2026  B.Tech, CSE(CSM)   \n",
            "\n",
            "                                  NAGAM MONIKA PRIYA       Unnamed: 3  \n",
            "0                  Email: monikapriyanagam@gmail.com              NaN  \n",
            "1                                  Number:9989527325              NaN  \n",
            "2                                          EDUCATION              NaN  \n",
            "3                                          Institute  CGPA/Percentage  \n",
            "4  Kakinada institute of engineering and technolo...            7.31%  \n",
            "\n",
            "Columns: ['Unnamed: 0', 'Unnamed: 1', 'NAGAM MONIKA PRIYA', 'Unnamed: 3']\n",
            "Error: DataFrame not loaded or 'Resume' column missing. Skipping preprocessing.\n",
            "\n",
            "Train/Test split complete. Shapes:\n",
            "  X_train: (769, 3000)\n",
            "  X_test: (193, 3000)\n",
            "  y_train_encoded: (769,)\n",
            "  y_test_encoded: (193,)\n",
            "\n",
            "Number of classes: 25\n",
            "Labels one-hot encoded.\n",
            "  y_train_categorical: (769, 25)\n",
            "  y_test_categorical: (193, 25)\n",
            "\n",
            "ANN Model built and compiled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m384,128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m1,625\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">384,128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,625</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m394,009\u001b[0m (1.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">394,009</span> (1.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m394,009\u001b[0m (1.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">394,009</span> (1.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting model training...\n",
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.3150 - loss: 3.1780 - val_accuracy: 0.5649 - val_loss: 3.0045\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.7241 - loss: 2.8129 - val_accuracy: 0.6234 - val_loss: 2.4684\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7641 - loss: 2.0267 - val_accuracy: 0.7143 - val_loss: 1.7099\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8897 - loss: 1.1776 - val_accuracy: 0.8961 - val_loss: 1.0413\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9733 - loss: 0.6245 - val_accuracy: 1.0000 - val_loss: 0.5662\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.3266 - val_accuracy: 1.0000 - val_loss: 0.3054\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.1397 - val_accuracy: 1.0000 - val_loss: 0.1882\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0870 - val_accuracy: 1.0000 - val_loss: 0.1177\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0509 - val_accuracy: 1.0000 - val_loss: 0.0888\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0368 - val_accuracy: 1.0000 - val_loss: 0.0663\n",
            "Model training finished.\n",
            "\n",
            "Evaluating model on test data...\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9973 - loss: 0.0540 \n",
            "Test Loss: 0.0645\n",
            "Test Accuracy: 99.48%\n",
            "\n",
            "Sample resume for prediction: 'I have experience in data science and Python'\n",
            "Cleaned sample resume: 'i have experience in data science and python'\n",
            "Sample resume vectorized. Shape: (1, 3000)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "Prediction probabilities (for all categories): [0.01274619 0.0310044  0.04811576 0.0231017  0.03924389 0.03077827\n",
            " 0.39023495 0.02975944 0.02258612 0.01300219 0.05527331 0.01155711\n",
            " 0.02074696 0.02223613 0.02783739 0.00498806 0.03656936 0.01014949\n",
            " 0.01012482 0.00441075 0.07907158 0.02846214 0.02361293 0.00621736\n",
            " 0.01816976]\n",
            "Predicted class index: 6\n",
            "\n",
            "Predicted Category: Data Science\n",
            "Shortlisted ✅\n",
            "Confidence Score for Predicted Category: 0.3902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved successfully as resume_classification_model.h5\n",
            "\n",
            "Calculating skill match for sample resume:\n",
            "Skills matched: 2 ➤ ['python', 'data science']\n"
          ]
        }
      ]
    }
  ]
}